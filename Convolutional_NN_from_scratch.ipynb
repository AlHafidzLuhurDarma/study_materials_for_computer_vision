{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "nPq3sAJTfIQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I6NoMLkeVsE"
      },
      "outputs": [],
      "source": [
        "input = np.random.randn(10, 1, 28, 28)  # 10 images, 1 channel, grayscale, 28 * 28 pixel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encoding label\n",
        "\n",
        "label = np.array([1, 3, 1, 2, 2, 3, 1, 1, 3 , 1])\n",
        "one_hot = np.zeros((label.size, label.max() + 1))\n",
        "one_hot[np.arange(label.size), label] = 1\n",
        "one_hot_transpose = one_hot.T\n",
        "one_hot_transpose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gJ3414lR9hs",
        "outputId": "0ddcc317-f516-4440-c6ce-11e8906d1943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
              "       [0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 1., 0., 0., 1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's make the convolutional part"
      ],
      "metadata": {
        "id": "N6k-LCALfc0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m, n_C, n_H_prev, n_W_prev = input.shape\n",
        "print(m, n_C, n_H_prev, n_W_prev)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKiHY3qvfgrq",
        "outputId": "565c4675-cd7e-4a12-839f-a9d15458e45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 1 28 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parameters"
      ],
      "metadata": {
        "id": "m6L-Q4YcgOGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filter_amount = 6\n",
        "filter_size = 3\n",
        "padding = 0\n",
        "stride = 1\n",
        "channel_amount = 1"
      ],
      "metadata": {
        "id": "nnuI-jnvgRzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output dimension"
      ],
      "metadata": {
        "id": "WqXxxElyf_Mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_number_channel = filter_amount\n",
        "new_height = int((n_H_prev + 2 * padding - filter_size)/ stride) + 1\n",
        "new_weight = int((n_W_prev + 2 * padding - filter_size)/ stride) + 1\n",
        "\n",
        "out_conv = np.zeros((m, new_number_channel, new_height, new_weight))"
      ],
      "metadata": {
        "id": "xtFzpApIgD3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(out_conv.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wZXM8vHhUGS",
        "outputId": "300c7cb9-516e-40cc-b496-bcfb1f87ebb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 6, 26, 26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "forward convolution"
      ],
      "metadata": {
        "id": "QP-JkIxrhnN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "bound = 1/math.sqrt(filter_size * filter_size)\n",
        "W = np.random.uniform(-bound, bound, size=(filter_amount, channel_amount, filter_size, filter_size))\n",
        "W_output = np.zeros((filter_amount, channel_amount, filter_size, filter_size))\n",
        "\n",
        "bias = np.random.uniform(-bound, bound, size=(filter_amount))\n",
        "bias_output = np.zeros((filter_amount))"
      ],
      "metadata": {
        "id": "UnVC0PGgjGRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images in range(m):\n",
        "  for channel in range(new_number_channel):\n",
        "    for height in range(new_height):\n",
        "      height_start = height * stride\n",
        "      height_end = height_start + filter_size\n",
        "      for weight in range(new_weight):\n",
        "        weight_start = weight * stride\n",
        "        weight_end = weight_start + filter_size\n",
        "\n",
        "        out_conv[images, channel, height, weight] = np.sum(input[images, :, height_start:height_end, weight_start:weight_end] * W[channel, ...]) + bias[channel]\n"
      ],
      "metadata": {
        "id": "kCQjGsUfk-i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFKJfhE3lXD-",
        "outputId": "e7aa8051-3d3b-4974-f1f8-80fbfa482253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 1, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_conv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEYBr3_ulJZ2",
        "outputId": "1df047d8-e476-41c7-dee2-81a76a5c880e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 6, 26, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#THE MAIN GOAL: To get the correct dimmension"
      ],
      "metadata": {
        "id": "6wGD050Ilq2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation for convolutional"
      ],
      "metadata": {
        "id": "Tms8B0SfmSHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_conv = np.tanh(out_conv)"
      ],
      "metadata": {
        "id": "feUJJd-mmgp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "forward average pooling"
      ],
      "metadata": {
        "id": "SYl_QbcymoqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m, n_C, n_H_prev, n_W_prev = out_conv.shape\n",
        "\n",
        "new_number_channel = filter_amount\n",
        "new_height = int((n_H_prev + 2 * padding - filter_size)/ stride) + 1\n",
        "new_weight = int((n_W_prev + 2 * padding - filter_size)/ stride) + 1\n",
        "\n",
        "out_pool = np.zeros((m, new_number_channel, new_height, new_weight))\n",
        "\n",
        "for images in range(m):\n",
        "  for channel in range(new_number_channel):\n",
        "    for height in range(new_height):\n",
        "      height_start = height * stride\n",
        "      height_end = height_start + filter_size\n",
        "      for weight in range(new_weight):\n",
        "        weight_start = weight * stride\n",
        "        weight_end = weight_start + filter_size\n",
        "\n",
        "        out_pool[images, channel, height, weight] = np.mean(out_conv[images, channel, height_start:height_end, weight_start:weight_end])"
      ],
      "metadata": {
        "id": "Yesv5IhOmyC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_conv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suGZyF8foYpu",
        "outputId": "f90f8382-6d81-465c-beac-cce7341eb28d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 6, 26, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(out_pool.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVs8QR_KoR4V",
        "outputId": "1adf4343-b0cc-4052-e7cc-d0ff96879fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 6, 24, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward Fully Connected layer"
      ],
      "metadata": {
        "id": "KImCThTLPDIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i, c, width, height = out_pool.shape\n",
        "width = width * height * c       # Flatten the data, 28 * 28 pixel with 1 dimension --> 1 dimensional array (28*28*1)\n",
        "height = 4                       # the amount of neuron\n",
        "\n",
        "bound = 1/np.sqrt(width)\n",
        "W_fc = np.random.uniform(low=-bound, high=bound, size=(height, width))\n",
        "b_fc = np.random.uniform(low=-bound, high=bound, size=(1, height))\n",
        "\n",
        "out_pool_flat = out_pool.reshape(10, -1)\n",
        "A_fc = np.dot(out_pool_flat, W_fc.T) + b_fc"
      ],
      "metadata": {
        "id": "E2-DXtAIPGrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_fc.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKO3wZVIRTuZ",
        "outputId": "c122319f-a92f-4b90-8856-8c10600beb09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "probability distribution"
      ],
      "metadata": {
        "id": "RaaMq54LRcsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = np.exp(A_fc) / np.sum(np.exp(A_fc))"
      ],
      "metadata": {
        "id": "5PPh1JvERnEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGiKavU8RvZ6",
        "outputId": "ab594500-b84d-445f-b728-2f1d20e24729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculate the loss / error"
      ],
      "metadata": {
        "id": "UdLTeASBRx36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = softmax.shape[1]\n",
        "deltaL = softmax - one_hot        # derivative value fo the loss\n",
        "loss = -np.sum(one_hot * np.log(softmax)) / batch_size # negative log-likelihood"
      ],
      "metadata": {
        "id": "41VC9dJRR2Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CWF6ZwETC_2",
        "outputId": "21bfca7b-f3a6-4e26-91f7-5a29794bb3f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.326551912935091"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backprop CNN"
      ],
      "metadata": {
        "id": "DS3Us7WcTHIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fcc backprop"
      ],
      "metadata": {
        "id": "ttxEztQpUBos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "g_W_fcc = 1/m * np.dot(deltaL.T, out_pool_flat)\n",
        "g_b_fcc = 1/m * np.sum(deltaL, axis=0)\n",
        "\n",
        "new_deltaL = np.dot(deltaL, W_fc)"
      ],
      "metadata": {
        "id": "8IsYsofqTKe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_deltaL.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNEFWyzfT3Mx",
        "outputId": "52d951d2-507d-4a42-9ae1-68281c1367ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 3456)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "average_pool backprop"
      ],
      "metadata": {
        "id": "AtNPjvrTT6Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_conv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCRNJQ--UYsZ",
        "outputId": "c3686115-713d-4442-9e6a-fdc28f45e6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 6, 26, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_pool.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALRCQCxHV41B",
        "outputId": "ad96b89d-6d36-465c-a0cd-4c12576ae382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 6, 24, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dout = np.reshape(new_deltaL, out_pool.shape) # Loss value shape ==> output of the pooling layer"
      ],
      "metadata": {
        "id": "JA6s4QfQUH4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, n_C, n_H, n_W = dout.shape"
      ],
      "metadata": {
        "id": "Dln2NbmkUxqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dX = np.zeros(out_conv.shape)\n",
        "\n",
        "for image in range(m):\n",
        "  for channel in range(n_C):\n",
        "    for height in range(n_H):\n",
        "      height_start = height * stride\n",
        "      height_end = height_start + filter_size\n",
        "      for weight in range(n_W):\n",
        "        weight_start = weight * stride\n",
        "        weight_end = weight_start + filter_size\n",
        "\n",
        "        # We're using average pooling layer\n",
        "        average = dout[image, channel, height, weight] / (filter_size * filter_size)\n",
        "        filter_average = np.full((filter_size, filter_size), average)\n",
        "        dX[image, channel, height_start:height_end, weight_start:weight_end] += filter_average"
      ],
      "metadata": {
        "id": "6C7wGKJSWeRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dX.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIvtMPbzYB6O",
        "outputId": "3005cd04-946c-45ed-d23e-6a23807289eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 6, 26, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dout.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3svcUrUYFfN",
        "outputId": "c6edcb09-a20d-4d61-a112-6e4da9734476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 6, 24, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TanH derivative"
      ],
      "metadata": {
        "id": "LSIrFDnfYOkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "back_tanh = (1 - np.tanh(out_conv)**2)"
      ],
      "metadata": {
        "id": "z3RaMx_iYWFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "convlutional backprop"
      ],
      "metadata": {
        "id": "lz-n4UHlYcCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m, n_C, n_H, n_W = input.shape   # (10, 1, 28, 28)\n",
        "m, n_C_dout, n_H_dout, n_W_dout = dout.shape # shape after tanh (10, 6, 26, 26)  -> output shape after forward conv\n",
        "\n",
        "dX = np.zeros(input.shape)\n",
        "W_grad = np.zeros((filter_amount, channel_amount, filter_size, filter_size))\n",
        "b_grad = np.zeros((filter_amount))\n",
        "\n",
        "# comput dW\n",
        "for image in range(m):\n",
        "  for channel in range(n_C_dout):\n",
        "    for height in range(n_H_dout):\n",
        "      height_start = height * stride\n",
        "      height_end = height_start + filter_size\n",
        "      for width in range(n_W_dout):\n",
        "        width_start = width * stride\n",
        "        width_end = width_start + filter_size\n",
        "\n",
        "        W_grad[channel, ...] += dout[image, channel, height, width] * input[image, :, height_start:height_end, width_start:width_end]\n",
        "\n",
        "        # gradient for dX\n",
        "        dX[image, :, height_start:height_end, width_start:width_end] += dout[image, channel, height, width] * W[channel, ...]\n",
        "\n",
        "# computer db\n",
        "for filter in range(filter_amount):\n",
        "  b_grad[filter, ...] = np.sum(dout[:, filter, ...])\n"
      ],
      "metadata": {
        "id": "1hw3Ij5lYj9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b_grad.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZHCTo2cZc59",
        "outputId": "0667c0bc-8274-4a9d-c65a-9228d0616c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6,)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W_grad.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iURepXoUasHd",
        "outputId": "7a41bbe2-2db6-4e2b-c8dd-804f09a4f833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1, 3, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dX.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtmFodRyavlg",
        "outputId": "a9e832bc-8250-4503-9ec0-b3ce7584bd47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dX == original input dimension shape"
      ],
      "metadata": {
        "id": "w1ouY81baziP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's put everything together"
      ],
      "metadata": {
        "id": "9jsm6xesa6Xl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MNIST Implementation of CNN from scratch"
      ],
      "metadata": {
        "id": "Lfw1LY0DU5rJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why implementing MNIST dataset for CNN:\n",
        "\n",
        "\n",
        "\n",
        "1.   To understand the intuition behind CNN(Convolutional Neural Network)\n",
        "2.   There's going to be a big problem at the end of this aprticular program (MNIST implementation for CNN)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZOBftFqOVK_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import some dependencies\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/mnist_train.csv\")\n",
        "data = np.array(data)\n",
        "batch_size = 32\n",
        "img, n = data.shape\n",
        "m = 1200\n",
        "np.random.shuffle(data)\n",
        "\n",
        "# m = amount of data we're using\n",
        "# n = amount of pixel\n",
        "\n",
        "data_dev = data[0:1000].T\n",
        "Y_dev = data_dev[0]\n",
        "X_dev = data[1:n]\n",
        "X_dev = X_dev / 255  # normalize the data\n",
        "\n",
        "data_train = data[1000:m].T\n",
        "Y_train = data_train[0]\n",
        "X_train = data_train[1:n]\n",
        "X_train = X_train / 255\n",
        "\n",
        "\n",
        "_, m_train = X_train.shape\n",
        "\n",
        "\n",
        "# One hot encoding for the label\n",
        "Y_train = Y_train[:batch_size]   # size of a batch\n",
        "one_hot_Y = np.zeros((Y_train.size, Y_train.max() + 1))\n",
        "one_hot_Y[np.arange(Y_train.size), Y_train] = 1\n",
        "one_hot = one_hot_Y.T\n",
        "\n",
        "X_train_reshape = X_train.T.reshape(-1, 1, 28, 28)\n",
        "\n",
        "# split the reshape data into batches of size 32\n",
        "batches = [X_train_reshape[i:i+batch_size] for i in range(0, X_train_reshape.shape[0], batch_size)]\n",
        "\n",
        "# 32, 1, 28, 28 --> batch size(amount of data inside 1 batch), dimension of the image, w, h\n",
        "print(batches[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ7By2u3VjeU",
        "outputId": "2b989491-f6fc-40f6-85e7-1f02990bfde7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 1, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple CNN"
      ],
      "metadata": {
        "id": "QcyJ2blgYYHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filter_amount = 6\n",
        "channel_amount = 1\n",
        "filter_size = 3\n",
        "stride = 1\n",
        "padding = 0\n",
        "epoch = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "input = batches[0]\n",
        "\n",
        "# learnable params\n",
        "# conv\n",
        "bound = 1/math.sqrt(filter_size * filter_size)\n",
        "W = np.random.uniform(-bound, bound, size=(filter_amount, channel_amount, filter_size, filter_size))\n",
        "bias = np.random.uniform(-bound, bound, size=(filter_amount))\n",
        "\n",
        "#fully connected\n",
        "width = 6 * 26 * 26\n",
        "height = 32\n",
        "bound = 1/np.sqrt(width)\n",
        "W_fc = np.random.uniform(low=-bound, high=bound, size=(height, width))\n",
        "b_fc = np.random.uniform(low=-bound, high=bound, size=(1, height))\n",
        "\n",
        "# conv grad\n",
        "W_grad = np.zeros((filter_amount, channel_amount, filter_size, filter_size))\n",
        "b_grad = np.zeros((filter_amount))\n",
        "\n",
        "  # fully connected layer initialization\n",
        "  # First Fully Connected Layer Initialization\n",
        "i, c, width, height = out_pool.shape\n",
        "width = width * height * c\n",
        "hidden_neurons = 32\n",
        "\n",
        "  # Initialize weights and biases for the first fully connected layer (tanh)\n",
        "bound_fc1 = 1/np.sqrt(width)\n",
        "W_fc1 = np.random.uniform(low=-bound_fc1, high=bound_fc1, size=(hidden_neurons, width))\n",
        "b_fc1 = np.random.uniform(low=-bound_fc1, high=bound_fc1, size=(1, hidden_neurons))\n",
        "\n",
        "  # Second Fully Connected Layer (10 neurons for classification)\n",
        "output_neurons = 10\n",
        "bound_fc2 = 1/np.sqrt(hidden_neurons)\n",
        "W_fc2 = np.random.uniform(low=-bound_fc2, high=bound_fc2, size=(output_neurons, hidden_neurons))\n",
        "b_fc2 = np.random.uniform(low=-bound_fc2, high=bound_fc2, size=(1, output_neurons))\n",
        "\n",
        "\n",
        "# training\n",
        "for i in range(epoch):\n",
        "  # conv initialization\n",
        "  m, n_C_prev, n_H_prev, n_W_prev = input.shape\n",
        "  new_number_channel = filter_amount\n",
        "  new_height = int((n_H_prev + 2 * padding - filter_size)/ stride) + 1\n",
        "  new_weight = int((n_W_prev + 2 * padding - filter_size)/ stride) + 1\n",
        "\n",
        "  out_conv = np.zeros((m, new_number_channel, new_height, new_weight))\n",
        "\n",
        "  # conv forward\n",
        "  for images in range(m):\n",
        "    for channel in range(new_number_channel):\n",
        "      for height in range(new_height):\n",
        "        height_start = height * stride\n",
        "        height_end = height_start + filter_size\n",
        "        for weight in range(new_weight):\n",
        "          weight_start = weight * stride\n",
        "          weight_end = weight_start + filter_size\n",
        "\n",
        "          out_conv[images, channel, height, weight] = np.sum(input[images, :, height_start:height_end, weight_start:weight_end] * W[channel, ...]) + bias[channel]\n",
        "\n",
        "  # tanh activation\n",
        "  out_conv_tanh = np.tanh(out_conv)\n",
        "\n",
        "  # average pooling initialization\n",
        "  m, n_C_prev, n_H_prev, n_W_prev = out_conv.shape\n",
        "  new_number_channel = filter_amount\n",
        "  new_height = int((n_H_prev + 2 * padding - filter_size)/ stride) + 1\n",
        "  new_weight = int((n_W_prev + 2 * padding - filter_size)/ stride) + 1\n",
        "  out_pool = np.zeros((m, new_number_channel, new_height, new_weight))\n",
        "\n",
        "  # average pooling forward\n",
        "  for images in range(m):\n",
        "    for channel in range(new_number_channel):\n",
        "      for height in range(new_height):\n",
        "        height_start = height * stride\n",
        "        height_end = height_start + filter_size\n",
        "        for weight in range(new_weight):\n",
        "          weight_start = weight * stride\n",
        "          weight_end = weight_start + filter_size\n",
        "\n",
        "          out_pool[images, channel, height, weight] = np.mean(out_conv[images, channel, height_start:height_end, weight_start:weight_end])\n",
        "\n",
        "\n",
        "  # fc layer\n",
        "  out_pool_flat = out_pool.reshape(m, -1)  # m is the batch size (32 here)\n",
        "  A_fc1 = np.dot(out_pool_flat, W_fc1.T) + b_fc1\n",
        "  Z_fc1 = np.tanh(A_fc1)  # Apply tanh activation\n",
        "\n",
        "  A_fc2 = np.dot(Z_fc1, W_fc2.T) + b_fc2\n",
        "\n",
        "  # Softmax Activation for Output Layer\n",
        "  softmax = np.exp(A_fc2) / np.sum(np.exp(A_fc2), axis=1, keepdims=True)\n",
        "\n",
        "  # Loss\n",
        "  batch_size = softmax.shape[0]\n",
        "  deltaL = softmax - one_hot.T  # delta for output layer (one_hot is transposed here)\n",
        "  loss = -np.sum(one_hot.T * np.log(softmax + 1e-8)) / batch_size  # add a small constant to avoid log(0)\n",
        "  print(f\"Loss: {loss}\")\n",
        "\n",
        "  # Backpropagation through the second fully connected layer (softmax layer)\n",
        "  g_W_fc2 = 1/m * np.dot(deltaL.T, Z_fc1)  # Gradient for W_fc2\n",
        "  g_b_fc2 = 1/m * np.sum(deltaL, axis=0)   # Gradient for b_fc2\n",
        "\n",
        "  # Backpropagate the error to the first fully connected layer\n",
        "  new_deltaL = np.dot(deltaL, W_fc2) * (1 - Z_fc1 ** 2)  # Derivative of tanh activation\n",
        "\n",
        "  # Backpropagation through the first fully connected layer (tanh layer)\n",
        "  g_W_fc1 = 1/m * np.dot(new_deltaL.T, out_pool_flat)  # Gradient for W_fc1\n",
        "  g_b_fc1 = 1/m * np.sum(new_deltaL, axis=0)\n",
        "\n",
        "  # average pool backprop\n",
        "\n",
        "  new_deltaL = np.dot(W_fc1.T, new_deltaL)\n",
        "\n",
        "  delta_out = np.reshape(new_deltaL, (32, 6, 24, 24))\n",
        "  m, n_C, n_H, n_W = delta_out.shape\n",
        "  dX = np.zeros((32, 6, 26, 26))\n",
        "\n",
        "  for image in range(m):\n",
        "    for channel in range(n_C):\n",
        "      for height in range(n_H):\n",
        "        height_start = height * stride\n",
        "        height_end = height_start + filter_size\n",
        "        for width in range(n_W):\n",
        "          width_start = width * stride\n",
        "          width_end = width_start + filter_size\n",
        "\n",
        "          average = delta_out[image, channel, height, width] / (filter_size * filter_size)\n",
        "          filter_average = np.full((filter_size, filter_size), average)\n",
        "          dX[image, channel, height_start:height_end, width_start:width_end] += filter_average\n",
        "\n",
        "  # convolution backprop\n",
        "\n",
        "  # tanh derivative\n",
        "  dX *= (1 - (np.tanh(out_conv)**2))\n",
        "\n",
        "  m, n_C, n_H, n_W = input.shape #(10, 1, 28, 28)\n",
        "  m, n_C_dout, n_H_dout, n_W_dout = delta_out.shape #shape after tanh (10, 6, 26, 26)\n",
        "\n",
        "  dX = np.zeros(input.shape)\n",
        "\n",
        "  #compute dW\n",
        "  for image in range(m):\n",
        "    for channel in range(n_C_dout):\n",
        "      for height in range(n_H_dout):\n",
        "        height_start = height * stride\n",
        "        height_end = height_start + filter_size\n",
        "        for width in range(n_W_dout):\n",
        "          width_start = width * stride\n",
        "          width_end = width_start + filter_size\n",
        "\n",
        "          W_grad[channel, ...] += delta_out[image, channel, height, width] * input[image, :, height_start:height_end, width_start:width_end]\n",
        "\n",
        "          dX[image, :, height_start:height_end, width_start:width_end] += delta_out[image, channel, height, width] * W[channel, ...]\n",
        "  #compute db\n",
        "  for filter in range(filter_amount):\n",
        "    b_grad[filter, ...] = np.sum(delta_out[:, filter, ...])\n",
        "\n",
        "  # update paramaters\n",
        "  W_fc2 -= learning_rate * g_W_fc2\n",
        "  b_fc2 -= learning_rate * g_b_fc2\n",
        "  W_fc1 -= learning_rate * g_W_fc1\n",
        "  b_fc1 -= learning_rate * g_b_fc1\n",
        "  W -= learning_rate * W_grad\n",
        "  bias -= learning_rate * b_grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PCzIxDUoMJB",
        "outputId": "497150c9-39e3-492c-f1b9-ff47b7432976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.319146329829259\n",
            "Loss: 2.278899902328964\n",
            "Loss: 2.2405174853523837\n",
            "Loss: 2.2032321097388277\n",
            "Loss: 2.16631548171382\n",
            "Loss: 2.129038791926226\n",
            "Loss: 2.090632563337759\n",
            "Loss: 2.0502368604852474\n",
            "Loss: 2.0068583061320155\n",
            "Loss: 1.9593736045685013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we reduce the amount of data by using convolutional technique, we also add another calculation inside our neural network\n"
      ],
      "metadata": {
        "id": "iWpwQCyXobBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem : How can we improve the speed of the training, using the same technique (cnn)\n",
        "\n",
        "if we're adding a new technique, if it's a calculation, we basicly not reducing the speed\n",
        "\n",
        "the more calculation = more time"
      ],
      "metadata": {
        "id": "cK5yU3Pmo2JJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PYTORCH"
      ],
      "metadata": {
        "id": "Cn3AlpaDphC-"
      }
    }
  ]
}